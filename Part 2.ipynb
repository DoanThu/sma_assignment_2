{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "(2500, 5032)\n"
     ]
    }
   ],
   "source": [
    "# Part II no. 1\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# get all txt filenames in a folder\n",
    "doc_files = glob.glob(r'D:\\Programming Exercise 2\\resources\\documents\\*.txt')\n",
    "\n",
    "# method to read all lines in a document and return it as a string\n",
    "def read_all_lines(file):\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.read().replace('\\n', '')\n",
    "        #lines = f.read()\n",
    "    return lines\n",
    "\n",
    "# build doc corpus\n",
    "doc_corpus = []\n",
    "for fileName in doc_files:\n",
    "    text = read_all_lines(fileName)\n",
    "    doc_corpus.append(text)\n",
    "\n",
    "print(len(doc_corpus))\n",
    "# create vector space representations using standard TF-IDF weighting scheme\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_doc = vectorizer.fit_transform(doc_corpus)\n",
    "# print(vectorizer.get_feature_names())\n",
    "print(X_doc.shape)\n",
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5032)\n"
     ]
    }
   ],
   "source": [
    "# Part II no. 2\n",
    "# get all txt filenames in a folder\n",
    "query_files = glob.glob(r'D:\\Programming Exercise 2\\resources\\queries\\*.txt')\n",
    "\n",
    "# build query corpus\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "query_corpus = []\n",
    "\n",
    "for fileName in query_files:\n",
    "    text = read_all_lines(fileName)\n",
    "    clean_word_list = [word for word in text.split() if word not in stoplist] \n",
    "    text = \" \" \n",
    "    text = text.join(clean_word_list)\n",
    "    query_corpus.append(text)\n",
    "\n",
    "X_query = vectorizer.transform(query_corpus)\n",
    "# print(vectorizer.get_feature_names())\n",
    "print(X_query.shape)\n",
    "# print(X_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "query_num,doc_num=X_query.shape[0],X_doc.shape[0]\n",
    "cos_dis=np.zeros((query_num,doc_num))\n",
    "\n",
    "for i in range(query_num):\n",
    "    for j in range(doc_num):\n",
    "        cos_dis[i][j]=cosine_similarity(X_query[i],X_doc[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=np.zeros((query_num, 100))\n",
    "\n",
    "for i in range(query_num):\n",
    "    candidates=cos_dis[i]\n",
    "    cur_res=candidates.argsort()[::-1][:100]\n",
    "    res[i]=cur_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 155.  554. 1474. 1543. 1785. 1794. 1763. 1772. 2189.  528.  847. 1922.\n",
      "  1753.  539.  727.  726. 1963.  936.  390. 1086. 1090. 1104. 1643. 1177.\n",
      "  1110.  536. 1176. 1091.  489. 1098. 1644. 1087.  254.  357. 1088. 1676.\n",
      "   762.  201.  493.  401.  364. 2274.  760. 1677.  271. 1634.  272. 1649.\n",
      "  1783.  939.  765. 1697. 1371. 1301. 1549.  746.   50.  567.  102.  221.\n",
      "   301. 1115.  822. 1484.  829. 1548. 1365.  744. 1648.  520.  997.  423.\n",
      "  1124. 1660. 1366.  793. 2076.  830.  220. 1505. 1658.  504.  753.  831.\n",
      "   758. 1243. 1040.  412.  623. 1919.  190.  671. 1970.  994.  127. 1959.\n",
      "  1994.   99.  414.  792.]\n",
      " [ 254. 2274.  760.  765.   50.  221.  220. 1994.  768.  758.  127.  213.\n",
      "   764.  214.  122. 1473.  258.  333.  560.  575.  349.  350. 1475.  559.\n",
      "   430.  209.  336.  199.  162.  442.  538.  185.  436.  219.  558.  198.\n",
      "   215.  206.  208.  207.  202.  200. 1399. 1371.  433.  520.  659.  203.\n",
      "  1009.  205.  166.  211.  766.  172.  174. 1177.  193. 1176.  171.  175.\n",
      "  1993.  173.  596.  597. 1696.  547. 1418.  820.  163. 1992.  595.  594.\n",
      "  1403.  165. 1417.  567. 1093.  157. 1100.  125. 2188.   99. 1089. 1922.\n",
      "   518.  939.  504.  821.   97.  339. 2463. 1722.  271.  543.  338.  272.\n",
      "  1470.  337.  183.  542.]\n",
      " [2486. 1330. 1782. 1781. 1752. 2483. 1754. 1760.  634.  323. 1758. 1759.\n",
      "   712.  645.  601.  612. 1707.  590. 1846. 1751. 1138. 1701. 1779. 1756.\n",
      "  2055. 1139. 1823. 1712. 1857. 1768.  820.  310.  821. 1109. 1835. 1659.\n",
      "  1318. 1757. 1755. 1761.  623.  380.  276.   34. 1111.  185.  571. 1698.\n",
      "  2485.   93. 1662. 1724.  223.  533.  825. 1801. 1654. 1661. 1319.  619.\n",
      "   679.  355. 2123. 1714.  701.   51.  246.  834. 1746. 1067.  553. 1735.\n",
      "  1078.  523. 2185. 2090. 1697. 2112. 1247.  668. 1677.  447.  565.  112.\n",
      "  1946. 1709.  967. 1968.  192. 1920.  522. 2463.  142.   61.  277.  670.\n",
      "   197.  690. 1856. 1246.]]\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
